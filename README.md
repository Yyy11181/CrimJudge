# CrimJudge Evaluation Framework

## Project Overview

CrimJudge is a specialized evaluation framework for assessing the quality of criminal judgment documents generated by Large Language Models (LLMs). The framework provides multi-dimensional evaluation methods, including rule-based automated evaluation and LLM-based intelligent evaluation.

## Project Structure

```
CrimJudge/
├── data/                           # Data storage directory
│   ├── 单人单罪/                   # Single defendant single crime cases
│   │   ├── SLJA/                   # SLJA dataset
│   │   ├── cail/                   # CAIL dataset
│   │   ├── judge/                  # Judge dataset
│   │   └── utils/                  # Utility scripts
│   ├── 单人多罪/                   # Single defendant multiple crimes cases
│   │   ├── cail/
│   │   ├── judge/
│   │   └── utils/
│   └── 多被告/                     # Multiple defendants cases
│       ├── cmdl/                   # CMDL dataset
│       ├── judge/                  # Judge dataset
│       ├── multiljp/               # MultiLJP dataset
│       └── utils/
│
├── outcome_evaluation/             # Outcome evaluation module
│   ├── calc_cail.py               # CAIL dataset evaluation script
│   ├── calc_cmdl.py               # CMDL dataset evaluation script
│   ├── calc_judge.py              # Judge dataset evaluation script
│   ├── calc_multiljp.py           # MultiLJP dataset evaluation script
│   ├── calc_slja614.py            # SLJA dataset evaluation script
│   ├── crime_extraction.py        # Crime name extraction module
│   ├── judge_extraction.py       # Judgment information extraction module
│   ├── law_extraction.py          # Law article extraction module
│   ├── filter_no_predictions.py   # Filter samples without predictions
│   └── segment/                   # Text segmentation processing module
│
├── legal_appro/                    # Legal evaluation module
│   ├── eval/                      # LLM evaluation scripts
│   │   ├── llm_eval_new.py       # LLM evaluation main program
│   │   ├── make_prompt.py        # Generate evaluation prompts
│   │   ├── use_template.py       # Use evaluation templates
│   │   ├── filter.py             # Data filtering
│   │   └── template/              # Evaluation templates
│   │       └── evaluate.txt      # Evaluation criteria template
│   ├── lls-as-judge/             # LLM as evaluator
│   │   ├── analyze_scores_HandLLM.py  # Analyze human vs LLM scoring consistency
│   │   ├── calculate_krippendorff_alpha.py  # Calculate Krippendorff's Alpha
│   │   └── utils/                # Utility scripts
│   ├── excel/                    # Excel result processing
│   │   ├── plot.py               # Plotting scripts
│   │   └── plot_error.py         # Error analysis plotting
│   └── generate/                 # Generate evaluation data
│
└── pipeline/                      # Data processing pipeline
    ├── request.py                 # LLM API request script
    ├── request_faruiguan.py       # Farui API request
    ├── request_yibu.py            # Async API request
    └── prompt.txt                 # Prompt template
```

## Main Features

### 1. Outcome Evaluation (outcome_evaluation)

This module provides rule-based automated evaluation with the following metrics:

- **Crime Evaluation**: Calculates recall and precision for crime identification
- **Law Article Evaluation**: Calculates recall and precision for law article citation
- **Sentence Evaluation**: Calculates accuracy for sentence prediction
- **F1 Score**: Comprehensive evaluation metric

Supported evaluation scripts:
- [calc_cail.py](outcome_evaluation/calc_cail.py) - CAIL dataset evaluation
- [calc_cmdl.py](outcome_evaluation/calc_cmdl.py) - CMDL dataset evaluation
- [calc_judge.py](outcome_evaluation/calc_judge.py) - Judge dataset evaluation
- [calc_multiljp.py](outcome_evaluation/calc_multiljp.py) - MultiLJP dataset evaluation
- [calc_slja614.py](outcome_evaluation/calc_slja614.py) - SLJA dataset evaluation

### 2. Legal Evaluation (legal_appro)

This module provides LLM-based intelligent evaluation, using LLMs as evaluators to assess generated judgment documents across multiple dimensions.

#### Evaluation Dimensions

1. **Factual Accuracy**
   - Assesses whether the facts adopted in the document are completely consistent with the original case facts
   - Score range: 1-5

2. **Legal Coherence**
   - Evaluates the reasoning process for conviction and sentencing
   - Includes correspondence between crimes and law articles, and reasonableness of the argumentation process
   - Score range: 1-5

3. **Completeness of Sentencing Factors**
   - Evaluates whether the document comprehensively and accurately identifies and discusses all factors affecting sentencing
   - Score range: 1-5

4. **Language & Format Normality**
   - Evaluates the document's language style, professional terminology usage, and overall format
   - Score range: 1-5

#### Evaluation Process

1. **Generate Evaluation Prompts**: Use templates to generate evaluation prompts
2. **LLM Evaluation**: Call LLM API for evaluation
3. **Result Analysis**: Analyze evaluation results and calculate consistency metrics
4. **Visualization**: Generate charts to display evaluation results

### 3. Data Processing Pipeline (pipeline)

This module provides functionality to interact with LLM APIs for generating evaluation data.

Main features:
- Supports multiple API types (OpenAI, Zhipu, SiliconFlow, etc.)
- Asynchronous requests for improved processing efficiency
- Supports caching and retry mechanisms
- Batch processing and concurrency control

## Usage

### Running Outcome Evaluation

```bash
# Evaluate CMDL dataset
python outcome_evaluation/calc_cmdl.py --gen_file data/多被告/cmdl/v30324/output.jsonl --exp_file data/多被告/cmdl/final.jsonl

# Evaluate SLJA dataset
python outcome_evaluation/calc_slja614.py --gen_file data/单人单罪/SLJA/SLJA500_faheng.jsonl --exp_file data/单人单罪/SLJA/final.jsonl
```

### Running LLM Evaluation

```bash
# Generate evaluation prompts
python legal_appro/eval/make_prompt.py

# Run LLM evaluation
python legal_appro/eval/llm_eval_new.py

# Analyze evaluation results
python legal_appro/lls-as-judge/analyze_scores_HandLLM.py
```

### Requesting LLM to Generate Data

```bash
# Use OpenAI API
python pipeline/request.py --api_type openai --api_key YOUR_API_KEY --model_name gpt-4

# Use Zhipu API
python pipeline/request.py --api_type zhipu --api_key YOUR_API_KEY --model_name glm-4
```

## Output Files

Evaluation results will be generated in the same directory as the input files:

- `evaluation_results.txt` - Detailed evaluation results
- `no_prediction_samples.txt` - Samples without predictions (CMDL dataset only)

## Data Format

### Input Data Format

Generated data format (JSONL):
```json
{"input": "Case facts", "gen_ans": "Generated judgment document"}
```

Ground truth data format (JSONL):
```json
{
  "Fact": "Case facts",
  "outcomes": [
    {
      "judgment": [
        {
          "standard_accusation": "Crime name",
          "article": ["Law article"],
          "penalty": {"imprisonment": sentence}
        }
      ]
    }
  ],
  "relevant_articles": ["Relevant law articles"]
}
```

### Evaluation Result Format

Evaluation results include for each sample:
- Ground truth and predicted crime names
- Ground truth and predicted law articles
- Ground truth and predicted sentences
- Recall, precision, and F1 scores

## Dependencies

Main dependencies:
- Python 3.7+
- json, argparse, os, re
- tqdm (progress bar)
- pandas, numpy (data analysis)
- scipy (statistical analysis)
- aiohttp, httpx (asynchronous HTTP requests)
- openai (OpenAI API)
- dashscope (Alibaba Cloud API)

## Notes

1. Evaluation scripts will automatically generate output files in the same directory as the input files
2. LLM evaluation requires configuration of corresponding API keys
3. Data files must use UTF-8 encoding
4. Evaluation result files are appended, not overwritten

## Contributors

This project is used for legal AI research to evaluate the performance of large language models in legal document generation tasks.

## License

Please refer to the LICENSE file in the project root directory.
