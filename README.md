

# CrimJudge Evaluation Framework

## ğŸ“Œ Overview

**CrimJudge** is a comprehensive evaluation framework designed to assess the quality of criminal judgments generated by Large Language Models (LLMs). Recognizing the complexity of legal documents, this framework provides a hybrid evaluation approach:

1. **Rule-based Automated Evaluation:** Precise metrics for legal elements (accusations, articles, penalties).
2. **LLM-as-a-Judge Evaluation:** Multi-dimensional qualitative assessment of legal reasoning and document professionalism.

---

## ğŸ“‚ Project Structure

```text
CrimJudge/
â”œâ”€â”€ data/                           # Dataset Storage
â”‚   â”œâ”€â”€ Single-Defendant-Single-Crime/  # SLJA, CAIL, Judge datasets
â”‚   â”œâ”€â”€ Single-Defendant-Multi-Crime/   # CAIL, Judge datasets
â”‚   â””â”€â”€ Multi-Defendant/                # CMDL, Judge, MultiLJP datasets
â”‚
â”œâ”€â”€ outcome_evaluation/             # Rule-based Evaluation Module
â”‚   â”œâ”€â”€ calc_[dataset].py           # Evaluation scripts for specific datasets
â”‚   â”œâ”€â”€ crime_extraction.py         # Accusation extraction logic
â”‚   â”œâ”€â”€ judge_extraction.py         # Judgment info extraction
â”‚   â”œâ”€â”€ law_extraction.py           # Legal article extraction
â”‚   â””â”€â”€ segment/                    # Text segmentation utilities
â”‚
â”œâ”€â”€ legal_appro/                    # LLM-based Intelligent Evaluation
â”‚   â”œâ”€â”€ eval/                       # LLM evaluation core logic & prompts
â”‚   â”œâ”€â”€ lls-as-judge/               # Consistency analysis (Human vs. LLM)
â”‚   â”œâ”€â”€ excel/                      # Visualization and error analysis
â”‚   â””â”€â”€ generate/                   # Evaluation data generation
â”‚
â””â”€â”€ pipeline/                       # Inference & API Pipeline
    â”œâ”€â”€ request.py                  # Standard API requests (OpenAI, etc.)
    â”œâ”€â”€ request_yibu.py             # Asynchronous API requests
    â””â”€â”€ prompt.txt                  # System prompt templates

```

---

## ğŸš€ Key Features

### 1. Outcome Evaluation (Rule-based)

Automatically extracts and evaluates structural legal elements using Ground Truth comparison:

* **Accusation Assessment:** Recall, Precision, and F1 for predicted crimes.
* **Legal Article Assessment:** Accuracy of cited law articles.
* **Penalty Prediction:** Error margin analysis for imprisonment terms.

### 2. Legal appropriateness (LLM-as-a-Judge)

Leverages advanced LLMs to score generated judgments across four critical legal dimensions (Scale 1-5):

1. **Factual Accuracy:** Does the judgment strictly adhere to the provided case facts?
2. **Legal Coherence:** Is the reasoning process logical? Do the charges match the legal citations?
3. **Completeness of Sentencing Factors:** Are all aggravating/mitigating circumstances identified?
4. **Language & Format Normality:** Does it follow professional legal writing styles and Chinese court formats?

### 3. High-Efficiency Pipeline

* **Multi-Model Support:** Native support for OpenAI, Zhipu AI, etc.
* **Async Processing:** High-concurrency requests for large-scale dataset evaluation.
* **Robustness:** Built-in retry mechanisms and caching to handle API instabilities.

---

## ğŸ›  Usage

### Quantitative Outcome Evaluation

```bash
# Evaluate on the CMDL dataset
python outcome_evaluation/calc_cmdl.py \
    --gen_file data/Multi-Defendant/cmdl/output.jsonl \
    --exp_file data/Multi-Defendant/cmdl/final.jsonl

```

### LLM-based Qualitative Evaluation

```bash
# 1. Generate evaluation prompts
python legal_appro/eval/make_prompt.py

# 2. Execute LLM evaluation
python legal_appro/eval/llm_eval_new.py

# 3. Analyze Consistency (e.g., Krippendorff's Alpha)
python legal_appro/lls-as-judge/calculate_krippendorff_alpha.py

```

### Data Generation Pipeline

```bash
python pipeline/request.py \
    --api_type openai \
    --api_key YOUR_API_KEY \
    --model_name gpt-4-turbo

```

---

## ğŸ“Š Data Schema

### Input/Output Format (JSONL)

```json
{
  "input": "Case facts description...",
  "gen_ans": "The generated legal judgment text..."
}

```

### Ground Truth Format

```json
{
  "Fact": "Original case facts",
  "outcomes": [
    {
      "judgment": [
        {
          "standard_accusation": "Theft",
          "article": ["Article 264"],
          "penalty": {"imprisonment": 12}
        }
      ]
    }
  ]
}

```

---

## ğŸ“ Dependencies

* Python 3.7+
* `aiohttp`, `httpx` (Async requests)
* `pandas`, `numpy`, `scipy` (Data Analysis)
* `tqdm` (Progress tracking)
* `openai`, `dashscope` (Model APIs)

## âš–ï¸ License

Distributed under the MIT License. See `LICENSE` for more information.

---
## ğŸ”® Future Work
We aim to evolve CrimJudge from a static evaluation tool into a dynamic, cross-jurisdictional legal AI ecosystem:
1. Living Benchmark
We are establishing a living benchmark for continuous evaluation. This involves periodic re-assessment of emerging LLMs using our standardized framework, transforming the evaluation from a static snapshot into a dynamic, continuously updated process.
2. Cross-Jurisdictional Evaluation
To bridge the gap between different legal systems, we plan to expand our scope:
* International Datasets: Integrate global benchmarks such as the ECHR (European Court of Human Rights) dataset.
* Legal Tradition Adaptation: Tailor criteria for common-law contexts, specifically assessing whether LLM-generated reasoning aligns with precedent-based legal logic (e.g., US and UK systems).
3. Normative & Ethical Dimensions
Future versions will move beyond technical accuracy to focus on higher-level judicial values:
* Judicial Fairness & Ethics: Evaluating algorithmic bias and adherence to fundamental judicial principles.
* Expert-in-the-Loop: Integrating structured manual evaluations by legal experts to ensure AI-assisted decision-making is normatively grounded.
